**HelpMate AI Project** : Retrieval Augmented Generation System to build AI Chat Bot to answer user queries in the Insurance Domain.

**Problem Statement:-**
The goal of the project is to build a RAG system using frameworks such as LlamaIndex or LangChain that is capable of effectively and accurately answering dfferent user queries from various insurance policy documents available externally.

**Tools/Framework used in Project:-**
LangChain and LlamaIndex
Chroma DB
OpenAI

**Advantage of using LangChain and LlamaIndex Framework:**
LangChain framework helps to build workflows by chaining sequence of LLM module. It has capability to connect to various tools like Database, API's and hence it helps in making more interactive applications.
LlamaIndex framework helps to connect to external data sources making it more efficient to work on various types of data files. It supports document indexing and document retrieval that enhances RAG application to retrieve the most relevant data based of user query.
Thus, LangChain and LlamaIndex both together is a good option to build RAG application for building efficient retrieval system.

**System Design Flow:**

  **1. Data Loading**
  For HelpMate AI project, we downloaded the insurance documents from the module 'RAG Demonstration'. The download files are sourced from below link:
  https://cdn.upgrad.com/uploads/production/8e278245-506c-4c8c-9246-892280692919/Policy+Documents.zip.
  We have used pdfplumber library to load pdfs as downloaded documenst are pdf documents. pdfplumber tool helps to extract not only text but also parse tables content, images etc for efficient text similarity     
  search and retrieval.

  **2. Document chunking and Pre-Processing:**
  PDF document contains multiple pages. It has simple text content and also in tabular format.
  Each page content were pre-processed to generate simple text in list format. Table are replaced with concatenated lists for easy display of data in table format by LLM model.
  Each chunks consists ofsingle pages of pdf docuemnts. Total we have 199 chunks generated from total 7 pdf's.

**3. Building the query engine**
  The general process for creating the query_engine is:
    1.Create nodes from the documents
    2.Create embedding of Document chunks
    3. Loading Embeddings and Meatadats into Vector Database. Here, we have used ChromaDB Database.
    4. Run Query Engine to retrieve top-k similar documents of user query.
    5. Re-ranking of top 10 similar documents retrieved from Engine Query to refine similar search and obtain most relevant content using similarity score values.

**4. Augmented Generation of Response by LLM**
  Generate the response using the retrieved nodes and user query and using prompt engineering to generate human readable format answers.

**5. Creating Response Pipeline.**
  Response pipeline combines all the necessary steps to build a RAG pipeline. 
  It is initialized by function initialize_conv(), which calls Query engine module and Response Generation module return the query response from the query engine along with the supporting documents.

**6. Build a Testing Pipeline**
  Input a series of questions to the Q/A bot and store the responses along with the feedback from the user whether it's accurate or not.
  Based on feedback, we calculate the accuracy score of Chat BOT RAG.
  We create at least 10 questions and store them in the questions list to be queried by the RAG system using the testing_pipeline function.

**Analysis and Recommendation :**
  Out of all questions answered, the answer for question 6 does not provide the correct pdf document name.
  Out of 10 questions, 9 questions were correctly retrieved and generated by RAG. Accuracy score = (9/10)*100 = 90%.

  **Short-Fall:** It is observed that for few queries RAG system takes time to answer the question (more than 15 seconds) which is not a good performing model.
  **Suggestion for model improvement**: We can modify architecture in second layer(Search and Re-rank layer) to include Cache DataBase so that previously asked questions and retrieved document metadatas are   
    stored in   cache database. Now, system must first search in cache database when a query is asked by user so that if similar question were asked before then it can retrieve document without performing any 
    similarity search   in main Database. This would help to reduce response time of Chat Bot when similar questions were asked before by any user.
    Second advantage is that by using cache database it will also help to reduce computation cost as every time system need not to call API for similarity search.
    Third advantage is that it can handle multi user request traffic due to caching common questions during peak time.

**Below are the detailed steps to run the code file:**
1. Import required libraries.
2. Mount google drive and provide permission to access drive.
3. Create path /content/drive/MyDrive/HelpMate Project/ and upload API key text file.
4. Save openAI key in openai.api_key variable.
5. Inside path /content/drive/MyDrive/HelpMate Project/Policy Document, keep copy of all insurance policy pdf documents.
6. First layer is run that consists of loading pdf files and create chunks of all pdf files. 
7. Each chunk are single page of pdf to maintain the context of data in a single page. 
8. Chunked documents along with metadata of each chunks is saved in dataframe.
9. As part of text pre-processing, we need to verify if chunk length is not not more than 8000 else it will get truncated when creating embeddings.
10. Maximum length of chunk is less than 2000 and hence good to go for embedding.
11. Minimum length of chunk is kept as 25 as chunk size less than 25 consists of irrelevant data.
12. Final updated dataframe of chunks along with user query are input to second layer which is search and Rank layer.
13. Chroma DB is used to store embeddings and retrieve top 10 relevant document based on distance metric. Lesser the distance , more similar is the document to user query.
14. Top 10 similar documents are feed into Re-Rank layer which uses Cross-Encoder model to further rank top 10 documents based on similarity search. 
15. Cross-Encoder model outputs similar score value of document and query pair which ranges from postivie to negative value. This score value is further normalised to obtain values between 0-1 range. Higher the score, more similar is the document with user query.
16. Top 3 documents having highest normalised score along with user query is added in prompt engineering for LLM to generate human understandable response of user query.
17.As part of testing pipeline, feedback is taken form user after each response provided by LLM and accuracy score is calculated as (Correct Retrievals) / Total Queries Ã— 100.

**Below are some screenshots of Response provided by RAG model for user queries :-**

Screenshot1 :

![image](https://github.com/user-attachments/assets/1e86bee1-41d9-423d-bc38-4a409538f065)

Screenshot2:

![image](https://github.com/user-attachments/assets/7a75b30c-7873-4054-81a8-63373f7e8ac8)
